{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import requests \n",
    "import tqdm\n",
    "import tarfile\n",
    "from clint.textui import progress\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import json\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    \n",
    "    with open(save_path, 'wb') as fd:\n",
    "        total_length = int(r.headers.get('content-length'))\n",
    "        \n",
    "        for chunk in progress.bar(r.iter_content(chunk_size=chunk_size),\n",
    "                                  expected_size=(total_length/1024) + 1):\n",
    "            fd.write(chunk)\n",
    "            fd.flush()\n",
    "                                  \n",
    "    return\n",
    "\n",
    "def extract_file(fname,path='.'):\n",
    "    if fname.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(fname, \"r:gz\")\n",
    "        tar.extractall(path=path)\n",
    "        tar.close()\n",
    "    elif fname.endswith(\"tar\"):\n",
    "        tar = tarfile.open(fname, \"r:\")\n",
    "        tar.extractall(path=path)\n",
    "        tar.close()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files\n",
    "\n",
    "img_url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2010/VOCtrainval_03-May-2010.tar\"\n",
    "annot_url = \"http://roozbehm.info/pascal-parts/trainval.tar.gz\"\n",
    "\n",
    "save_path_img = \"\\\\img_zip\"\n",
    "save_path_annot = \"\\\\annot_zip\"\n",
    "\n",
    "curr_path = os.getcwd()\n",
    "path = Path(curr_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = os.path.join(path.parent,'datasets\\\\PASCAL-VOC'+save_path_img)\n",
    "os.mkdir(save_loc)\n",
    "download_url(img_url,os.path.join(save_loc,'images.tar'))\n",
    "\n",
    "save_loc = os.path.join(path.parent,'datasets\\\\PASCAL-VOC'+save_path_annot)\n",
    "os.mkdir(save_loc)\n",
    "download_url(annot_url,os.path.join(save_loc,'annotations.tar.gz'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = os.path.join(path.parent,'datasets\\\\PASCAL-VOC'+save_path_annot)\n",
    "extract_file(save_loc+'\\\\annotations.tar.gz',path = save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = os.path.join(path.parent,'datasets\\\\PASCAL-VOC'+save_path_img)\n",
    "extract_file(save_loc+'\\\\images.tar',path = save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class_list = os.listdir(save_loc+'\\\\VOCdevkit\\\\VOC2010\\\\ImageSets\\\\Main')\n",
    "# class_list = [file.split('_')[0] for file in class_list]\n",
    "# class_list = list(filter(lambda x: ('val' not in x)and('train' not in x),\n",
    "#                          class_list))\n",
    "# class_set = set(class_list)\n",
    "class_set = set(['aeroplane','bicycle','bird','cat','cow','dog','horse','motorbike','person','sheep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2008_000002\n",
      "1    2008_000003\n",
      "2    2008_000007\n",
      "3    2008_000008\n",
      "4    2008_000009\n",
      "Name: img, dtype: object\n"
     ]
    }
   ],
   "source": [
    "anno_destination = os.path.join(path.parent,'datasets\\\\PASCAL-VOC\\\\xybb-objects\\\\')\n",
    "anno_source = os.path.join(path.parent,('datasets\\\\PASCAL-VOC'+\n",
    "                                        save_path_annot+\n",
    "                                        '\\\\Annotations_Part\\\\'))\n",
    "img_source = os.path.join(path.parent,('datasets\\\\PASCAL-VOC'+\n",
    "                                       save_path_img+\n",
    "                                       '\\\\VOCdevkit\\\\VOC2010\\\\JPEGImages\\\\'))\n",
    "\n",
    "save_loc = os.path.join(path.parent,'datasets\\\\PASCAL-VOC'+save_path_img)\n",
    "for ob_class in class_set:\n",
    "                                       \n",
    "    new_dir = anno_destination+ob_class\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.mkdir(new_dir)\n",
    "    \n",
    "    if not os.path.exists(new_dir+'\\\\bbox'):\n",
    "        os.mkdir(new_dir+'\\\\bbox')\n",
    "        \n",
    "    if not os.path.exists(new_dir+'\\\\mask'):\n",
    "        os.mkdir(new_dir+'\\\\mask')\n",
    "        \n",
    "    if not os.path.exists(new_dir+'\\\\rgb'):\n",
    "        os.mkdir(new_dir+'\\\\rgb')\n",
    "    \n",
    "    if not os.path.exists(new_dir+'\\\\numpy'):\n",
    "        os.mkdir(new_dir+'\\\\numpy')\n",
    "    \n",
    "#for ob_class in class_set:\n",
    "#    print(ob_class)\n",
    "    \n",
    "img_list = pd.read_csv((save_loc+'\\\\VOCdevkit\\\\VOC2010\\\\ImageSets\\\\Main\\\\train_trainval.txt'),\n",
    "                      header=None,index_col=None,sep='\\t')\n",
    "\n",
    "\n",
    "img_list = img_list[0].str.split(' ',expand=True)\n",
    "img_list.columns = ['img','flag','drop']\n",
    "img_list.drop(['drop'],axis=1,inplace=True)\n",
    "\n",
    "for image in img_list.sample(n=100).img:\n",
    "    mat = scipy.io.loadmat(anno_source+image+'.mat')\n",
    "    img_file = (img_source+image+'.jpg')\n",
    "    img_dest = os.path.join(path.parent,'datasets\\\\PASCAL-VOC\\\\scene\\\\'+image+'.jpg')\n",
    "    \n",
    "    if not os.path.exists(img_dest):\n",
    "        shutil.move(img_file,img_dest)\n",
    "    label_list = []\n",
    "    annotation_dict = {}\n",
    "    item_idx = 0\n",
    "    for annotation in mat['anno'][0][0][1][0]:\n",
    "        annotation_dict = {}\n",
    "        label = annotation[0][0]\n",
    "        if label not in class_set:\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(anno_destination+label+'\\\\bbox\\\\'+image+'.json'):\n",
    "            \n",
    "            with open(anno_destination+label+'\\\\bbox\\\\'+image+'.json') as fp:\n",
    "                annotation_dict = json.load(fp)\n",
    "    \n",
    "            item_idx = max(map(int,annotation_dict.keys()))+1\n",
    "\n",
    "        annotation_dict[item_idx] = {}\n",
    "        annotation_dict[item_idx]['mask'] = annotation[2].tolist()\n",
    "        annotation_dict[item_idx]['bbox'] = np.append(np.min(np.where(annotation[2]==1),\n",
    "                                                                   axis=1),\n",
    "                                                   np.max(np.where(annotation[2]==1),\n",
    "                                                                   axis=1)).tolist()\n",
    "        annotation_dict[item_idx]['parts'] = {}\n",
    "        parts_dict = annotation_dict[item_idx]['parts'] \n",
    "\n",
    "        if (len(annotation[3])>0)and (len(annotation[3][0])>0):\n",
    "            for part in annotation[3][0]:\n",
    "\n",
    "                part_label = part[0][0]\n",
    "                parts_dict[part_label] = {}\n",
    "                parts_dict[part_label]['mask'] = part[1].tolist()\n",
    "                parts_dict[part_label]['bbox'] = np.append(np.min(np.where(part[1]==1),\n",
    "                                                                          axis=1),\n",
    "                                                       np.max(np.where(part[1]==1),\n",
    "                                                              axis=1)).tolist()\n",
    "\n",
    "        item_idx+=1\n",
    "\n",
    "\n",
    "\n",
    "        if len(annotation_dict.keys())>0:\n",
    "            with open(anno_destination+label+'\\\\bbox\\\\'+image+'.json', 'w') as fp:\n",
    "                json.dump(annotation_dict, fp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_labels = {'head':1, 'leye':2, 'reye':3, 'beak':4, 'torso':5, 'neck':6, 'lwing':7, 'rwing':8, 'lleg':9, 'lfoot':10, 'rleg':11, 'rfoot':12, 'tail':13}\n",
    "\n",
    "cat_labels = {'head':1, 'leye':2, 'reye':3, 'lear':4, 'rear':5, 'nose':6, 'torso':7, 'neck':8, 'lfleg':9, 'lfpa':10, 'rfleg':11, 'rfpa':12, 'lbleg':13, 'lbpa':14, 'rbleg':15, 'rbpa':16, 'tail':17}\n",
    "\n",
    "cow_labels = {'head':1, 'leye':2, 'reye':3, 'lear':4, 'rear':5, 'muzzle':6, 'lhorn':7, 'rhorn':8, 'torso':9, 'neck':10, 'lfuleg':11, 'lflleg':12, 'rfuleg':13, 'rflleg':14, 'lbuleg':15, 'lblleg':16, 'rbuleg':17, 'rblleg':18, 'tail':19}\n",
    "\n",
    "dog_labels = {'head':1, 'leye':2, 'reye':3, 'lear':4, 'rear':5, 'nose':6, 'torso':7, 'neck':8, 'lfleg':9, 'lfpa':10, 'rfleg':11, 'rfpa':12, 'lbleg':13, 'lbpa':14, 'rbleg':15, 'rbpa':16, 'tail':17, 'muzzle':18}\n",
    "\n",
    "horse_labels = {'head':1, 'leye':2, 'reye':3, 'lear':4, 'rear':5, 'muzzle':6, 'lfho':7, 'rfho':8, 'torso':9, 'neck':10, 'lfuleg':11, 'lflleg':12, 'rfuleg':13, 'rflleg':14, 'lbuleg':15, 'lblleg':16, 'rbuleg':17, 'rblleg':18, 'tail':19, 'lbho':20, 'rbho':21}\n",
    "9\n",
    "bottle_labels = {'cap':1, 'body':2}\n",
    "\n",
    "person_labels = {'head':1, 'leye':2,  'reye':3, 'lear':4, 'rear':5, 'lebrow':6, 'rebrow':7,  'nose':8,  'mouth':9,  'hair':10, 'torso':11, 'neck': 12, 'llarm': 13, 'luarm': 14, 'lhand': 15, 'rlarm':16, 'ruarm':17, 'rhand': 18, 'llleg': 19, 'luleg':20, 'lfoot':21, 'rlleg':22, 'ruleg':23, 'rfoot':24}\n",
    "\n",
    "bus_labels = { 'frontside':1, 'leftside':2, 'rightside':3, 'backside':4, 'roofside':5, 'leftmirror':6, 'rightmirror':7, 'fliplate':8, 'bliplate':9  }\n",
    "for ii in range(0,10):\n",
    "    bus_labels['door_{}'.format(ii+1)] = 10+ii\n",
    "for ii in range(0,10):\n",
    "    bus_labels['wheel_{}'.format(ii+1)] = 20+ii\n",
    "for ii in range(0,10):\n",
    "    bus_labels['headlight_{}'.format(ii+1)] = 30+ii\n",
    "for ii in range(0,20):\n",
    "    bus_labels['window_{}'.format(ii+1)] = 40+ii\n",
    "\n",
    "car_labels = { 'frontside':1, 'leftside':2, 'rightside':3, 'backside':4, 'roofside':5, 'leftmirror':6, 'rightmirror':7, 'fliplate':8, 'bliplate':9  }\n",
    "for ii in range(0,3):\n",
    "    car_labels['door_{}'.format(ii+1)] = 10+ii\n",
    "for ii in range(0,4):\n",
    "    car_labels['wheel_{}'.format(ii+1)] = 13+ii\n",
    "for ii in range(0,6):\n",
    "    car_labels['headlight_{}'.format(ii+1)] = 17+ii\n",
    "for ii in range(0,7):\n",
    "    car_labels['window_{}'.format(ii+1)] = 23+ii\n",
    "\n",
    "aeroplane_labels = {'body': 1, 'stern': 2, 'lwing': 3, 'rwing':4, 'tail':5}\n",
    "for ii in range(1, 10):\n",
    "    aeroplane_labels['engine_{}'.format(ii)] = 5+ii\n",
    "for ii in range(1, 10):\n",
    "    aeroplane_labels['wheel_{}'.format(ii)] = 14+ii\n",
    "\n",
    "motorbike_labels = {'fwheel': 1, 'bwheel': 2, 'handlebar': 3, 'saddle': 4}\n",
    "for ii in range(0,10):\n",
    "    motorbike_labels['headlight_{}'.format(ii+1)] = 5+ii\n",
    "motorbike_labels['body']=15\n",
    "\n",
    "bicycle_labels = {'fwheel': 1, 'bwheel': 2, 'saddle': 3, 'handlebar': 4, 'chainwheel': 5}\n",
    "for ii in range(0,10):\n",
    "    bicycle_labels['headlight_{}'.format(ii+1)] = 6+ii\n",
    "bicycle_labels['body']=16\n",
    "\n",
    "train_labels = {'head':1,'hfrontside':2,'hleftside':3,'hrightside':4,'hbackside':5,'hroofside':6}\n",
    "for ii in  range(0,10):\n",
    "    train_labels['headlight_{}'.format(ii+1)] = 7 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['coach_{}'.format(ii+1)] = 17 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['cfrontside_{}'.format(ii+1)] = 27 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['cleftside_{}'.format(ii+1)] = 37 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['crightside_{}'.format(ii+1)] = 47 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['cbackside_{}'.format(ii+1)] = 57 + ii\n",
    "for ii in  range(0,10):\n",
    "    train_labels['croofside_{}'.format(ii+1)] = 67 + ii\n",
    "\n",
    "sheep_labels = cow_labels\n",
    "\n",
    "part_labels = {'bird': bird_labels, 'cat': cat_labels, 'cow': cow_labels, 'dog': dog_labels, 'sheep': sheep_labels, 'horse':horse_labels, 'car':car_labels, 'bus':bus_labels, 'bicycle':bicycle_labels, 'motorbike':motorbike_labels, 'person':person_labels,'aeroplane':aeroplane_labels, 'train':train_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path.parent,'datasets\\\\PASCAL-VOC\\\\')+'part_label.json', 'w') as fp:\n",
    "    json.dump(part_labels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "for ob_class in class_set:\n",
    "    \n",
    "    part_mapping = part_labels[ob_class]\n",
    "    \n",
    "    img_list = os.listdir(anno_destination+ob_class+'\\\\bbox\\\\')\n",
    "    labels = [] #np.zeros([24,len(annotation_dict.keys())])\n",
    "    masks = []\n",
    "    bboxes = []\n",
    "    max_labels = 24\n",
    "    \n",
    "    for file in img_list:\n",
    "        \n",
    "        with open(anno_destination+ob_class+'\\\\bbox\\\\'+file) as fp:\n",
    "            annotation_dict = json.load(fp)\n",
    "        img_dest = os.path.join(path.parent,'datasets\\\\PASCAL-VOC\\\\scene\\\\'+file.split('.')[0]+'.jpg')\n",
    "        image_arr = cv2.imread(img_dest)\n",
    "\n",
    "        for ct, idx in enumerate(annotation_dict.keys()):\n",
    "            anno = annotation_dict[idx]\n",
    "            p_list = anno['parts'].keys()\n",
    "            label_arr = np.array([part_mapping[x] for x in p_list])\n",
    "            padding = max_labels-label_arr.shape[0]\n",
    "            label_arr = np.pad(label_arr,(0,padding),'constant',constant_values=(0))\n",
    "            labels.append(label_arr)\n",
    "            \n",
    "            bb_array = []\n",
    "            mask_array = []\n",
    "            if len(p_list)>0: \n",
    "                for p_key in p_list:\n",
    "\n",
    "                    bb_array.append(anno['parts'][p_key]['bbox'])\n",
    "                    #bb_mask = bb_array[0][0]:bb_array[0][2],bb_array[0][1]:bb_array[0][3]\n",
    "                    part = image_arr[bb_array[0][0]:bb_array[0][2],bb_array[0][1]:bb_array[0][3]]\n",
    "                    mask = part*np.expand_dims(np.array(anno['parts'][p_key]['mask'])[bb_array[0][0]:bb_array[0][2],bb_array[0][1]:bb_array[0][3]],-1)\n",
    "                    mask = cv2.resize(mask.astype('uint8'),(64,64))\n",
    "                    mask_array.append(mask)\n",
    "                bb_array = np.pad(bb_array,((0,padding),(0,0)),'constant',constant_values=(0))\n",
    "                mask_array = np.pad(mask_array,((0,padding),(0,0),(0,0),(0,0)),'constant',constant_values=(0))\n",
    "                bboxes.append(bb_array)\n",
    "                masks.append(mask_array)\n",
    "        \n",
    "        \n",
    "    #np.save(anno_destination+ob_class+'\\\\numpy\\\\'+ob_class+'.npy',labels)\n",
    "    #np.save(anno_destination+ob_class+'\\\\numpy\\\\'+ob_class+'.npy',labels)\n",
    "    #np.save(anno_destination+ob_class+'\\\\numpy\\\\'+ob_class+'.npy',labels)\n",
    "    \n",
    "    with open(anno_destination+ob_class+'\\\\numpy\\\\'+ob_class + '_part_separated_labels', 'wb') as f:\n",
    "        pickle.dump(np.array(labels), f)\n",
    "\n",
    "    with open(anno_destination+ob_class+'\\\\numpy\\\\'+ob_class + '_part_separated_bbx', 'wb') as f:\n",
    "        pickle.dump(np.array(bboxes), f)\n",
    "\n",
    "    with open(anno_destination+ob_class+'\\\\numpy\\\\'+ob_class + '_part_separated_masks', 'wb') as f:\n",
    "        pickle.dump(np.array(masks), f)\n",
    "\n",
    "                \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
