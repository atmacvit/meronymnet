# Metrics for quantitative Evaluation

To measure the performance of our generative model, we have implemented 6 commonly used metrics:
* Inception Score
* Frechet Inception Distance
* Modified Inception Score
* Diversity Score
* Activation Maximization Score
* Classification Accuracy Score

## How to use?

Package score contains implementation of 6 above mentioned scores apart from a module for utility functions.

The script metrics.py contains a class Metrics which can be imported and used to calculate the required scores. 

### Use as a script

```
python 'path/to/metrics.py' "$path_fake" --path_real "$path_real" --splits="$splits" --model "$model" --object_names "$object_names" --path_model "$path_model" --path_result "$path_result" --precision="$precision" --num_samples="$num_samples"
```

Saves a csv file of metrics and a JSON file of raw results in results folder.

#### Positional Arguments:
```
path_fake [string] ="path/to/generations_folder"
```
#### Optional Arguments:
```
path_real [string] = "path/to/training_data" (if not provided then FID, CAS and AMS are not calculated)

model [string] {'inception','resnet50'} = whether to use pretrained inceptionV3 or fine tuned resnet50 model (deafult='inception')

path_model [string] = "path/to/saved_model" [only used when model is not 'inception']

splits [int] = Number for splits per-class (default=10)

precision [int] = Number of decimal digits accurate to which results are calculated (default=4)

object_names [string] = comma separated string without any space like "cow,person,cat" (defaults to all object in generations directory)

scores [string] = comma separated string of score abbreviations without any space like "fid,mis,is,ds,cas,ams" (defaults to all scores)

path_result [string] = "path/to/result_file.csv" (path must have .csv extennsion, if path is not provided then a suitable path is inferred)

num_samples [int] = Number of samples of each class for which results are calculated (default = 500)
```

### Use as module
```
from metrics import Metrics

metric= Metrics(path_fake,path_real=None,model="inception",path_model=None,splits=10,object_names=None,num_samples=500) #Note: Here object_names should a list of objects and NOT a string

scores={'FID':None, 'MIS':None, 'IS':None, 'AMS':None, 'CAS':None, 'DS':None} #scores to calculate

for score_name in scores:
	scores[score_name]=metric.score_functions[score_name]()
	
#Each value in scores dictionary contains a dictionary with keys as object names as values as a tuple of mean and std
print(scores['FID']) 
```

## Inception Score

Inception Score offers a way to quantitatively evaluate the quality of generated samples. The score was motivated by the following considerations: 

1. The conditional label distribution of samples containing meaningful objects should have low entropy
2.  The variability of the samples should be high, or equivalently, the marginal R z p(y|x = G(z))dz should have high entropy.

Finally, these desiderata are combined into one score, 
IS(G) = exp(Ex∼G[dKL(p(y | x), p(y)]) = exp (H(y) − Ex [H(y|x)]) 

Exponentiation is performed so that results are easier to compare.
The classifier is Inception Net (version 3) trained on Image Net. The authors found that this score is well-correlated with scores from human annotators. Drawbacks include insensitivity to the prior distribution over labels and not being a proper distance.

## Frechet Inception Distance

FID quantifies the quality of generated samples by first embedding into a feature space given by (a specific layer) of Inception Net. Then, viewing the embedding layer as a continuous multivariate Gaussian, the mean and covariance is estimated for both the generated data and the real data. The Fréchet distance between these two Gaussians is then used to quantify the quality of the samples, i.e. 

FID(x, g) = ||µx − µg||2 2 + Tr(Σx + Σg − 2(ΣxΣg) 1 2 ),
 
where (µx, Σx), and (µg, Σg) are the mean and covariance of the sample embeddings from the data distribution and model distribution, respectfully. 

The authors show that the score is consistent with human judgment and more robust to noise than IS . Unlike IS, FID can detect intra-class mode dropping, i.e. a model that generates only one image per class can score a perfect IS, but will have a bad FID. A significant drawback of both IS and FID measures is the inability to detect overfitting. A “memory GAN” which stores all training samples would score perfectly. 

## Modified Inception Score

In its original formulation, “inception-score” assigns a higher score for models that result in a low entropy class conditional distribution p(y|x). However, it is desirable to have diversity within image samples of a particular category. To characterise this diversity, MIS useS a cross-entropy style score −p(y|xi)log(p(y|xj )) where xj s are samples of the same class as xi as per the outputs of the trained inception model. Incorporating this term into the original inception-score results in: 

MIS=exp(Exi [Exj [(KL(P(y|xi)||P(y|xj ))]]), 

which is calculated on a per-class basis and is then averaged over all classes. 

Essentially, m-IS can be viewed as a proxy for measuring both intra-class sample diversity as well as sample quality.

## Activation Maximization Score

The entropy term on H(y) in Inception Score is problematic when training data is not evenly distributed over classes, for that argmin H(y) is a uniform distribution. To take into account the class imbalance in training set, AM score replaces the H(y) term in Inception score with the KL divergence between y train and y. The AM score is then defined as 

KL(p(y train) k p(y))+Ex H(y|x) .

The AM score consists of two terms. The first one is minimized when y train is close to y. The second term is minimized when the predicted class label for sample x (i.e. y|x) has low entropy. The minimal value of AM Score is zero, and the smaller value, the better.

## Diversity Score

To measure the diversity of generated samples, DS takes into account both the inter-class, and the intra-class diversity. 

Intra-class diversity is measured by the average (negative) MS-SSIM metric between all pairs of generated images in a given set of generated images X: 
dintra(X) = 1 − 1 |X| 2 X (x,x0)∈X×X MS-SSIM(x, x 0 )

For inter-class diversity, a pre-trained classifier is used to classify the set of generated images, such that for each sampled image x, there is a classification prediction in the form of a one-hot vector c(x).  Then, the entropy of the average one-hot classification prediction vector is measured to evaluate the diversity between classes in the samples set: dinter(X) = 1 log(N) H 1 |X| X x∈X c(x) 

Finally, the diversity score is defined as the geometric mean of intra class diversity and inter-class diversity

d(X) = (p dintra(X) ∗ dinter(X))^(1/2)

## Classification Accuracy Score

CAS  is based on the idea that if the model captures the data distribution, performance on any downstream task should be similar whether using the original or model data.  Accordingly, from samples x, y ∼ p(y)pθ(x|y),  a discriminative model pˆ(y|x) to learn pθ(y|x), and it is used to estimate the expected posterior loss Epˆ(y|x) [L(y, yˆ)|X]. The generative risk is then defined as Ep(x,y) [L(y, yˆg)], where yˆg is the classifier that minimizes the expected posterior loss under pˆ(y|x). Then the performance of the classifier is compared to the performance of the classifier trained on samples from p(x, y). In the case of conditional generative models of images, y is the class label for image x, and the model of pˆ(y|x) is an image classifier.



